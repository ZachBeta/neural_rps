# Neural Rock Paper Scissors

A neural network-based implementation of Rock Paper Scissors using Go, focused on AlphaGo-style techniques.

## Overview

This project explores AlphaGo-style techniques applied to a strategic card game combining Rock Paper Scissors with board placement. It features:

- **AlphaGo-Style RPS Card Game**: Implementation using MCTS and neural networks (policy and value networks). Located primarily within the `alphago_demo/` directory.
- **Core Neural Network Package**: Reusable neural network components (CPU, GPU via gRPC) in the `pkg/neural/` directory.
- **Various Training and Evaluation Tools**: Including self-play training, ELO tournaments, minimax comparisons, and model analysis utilities within `alphago_demo/cmd/`.

## Project Organization

The project uses a single Go module defined at the root (`go.mod`).

```
.
├── alphago_demo/    # AlphaGo-style implementations (RPS Card Game, TicTacToe demo [legacy])
│   ├── cmd/         # Various command-line entry points (training, tournaments, etc.)
│   ├── pkg/         # Core game logic, agents, MCTS, analysis for the demo
│   └── ...
├── pkg/             # Core shared packages
│   ├── common/      # Common utilities
│   ├── neural/      # Neural network implementations (CPU, GPU client)
│   │   ├── cpu/
│   │   ├── gpu/
│   │   └── proto/
│   └── ...
├── cmd/             # Root-level commands
│   └── benchmark/   # Benchmarking tool for CPU/GPU network performance
├── output/          # Default directory for generated models and results
├── python/          # Python gRPC service for GPU acceleration (optional)
├── go.mod           # Go module definition
├── go.sum           # Go module checksums
├── README.md        # This file
└── ...              # Other configuration files
```

## Getting Started

### Building

The project uses Go modules. Ensure you have Go installed (version 1.18 or later recommended).

```bash
# Tidy dependencies (recommended after pulling changes)
go mod tidy

# Build a specific command (example)
go build ./alphago_demo/cmd/train_models

# Build the benchmark tool
go build ./cmd/benchmark
```

### Running Key Commands

Use `go run` to execute the main entry points. Some commands require trained models (which can be generated by `train_models`).

```bash
# Run the benchmark tool (may show errors if GPU service isn't running)
go run ./cmd/benchmark/main.go

# Run a quick training cycle (generates models in ./output/)
go run ./alphago_demo/cmd/train_models/main.go -small-run

# Run an ELO tournament (uses models from ./output/)
# Use -games flag for fewer games during testing
go run ./alphago_demo/cmd/elo_tournament/main.go -games 2

# Run the interactive RPS card game demo
# (Trains a temporary model first if none exist)
go run ./alphago_demo/cmd/rps_card/main.go

# Run the interactive Tic-Tac-Toe demo (legacy)
go run ./alphago_demo/cmd/tictactoe/main.go
```

## Tournament Systems

The project includes tournament systems to evaluate and compare different AI agents within the `alphago_demo`.

#### ELO Tournament System

Compares discovered models using ELO ratings.

```bash
# Run with default settings (100 games/pair)
go run ./alphago_demo/cmd/elo_tournament/main.go

# Run with fewer games and verbose output
go run ./alphago_demo/cmd/elo_tournament/main.go -games 10 -verbose
```

Options:
- `-games <n>`: Number of games per agent pair (default: 100)
- `-verbose`: Enable detailed output for each game
- `-cutoff <n>`: ELO threshold to prune underperforming agents (default: 1400, 0 to disable)
- `-output <file>`: Specify output file for results (default: output/tournament_results.csv)
- `-top <n>`: Only use top N agents from previous tournament results

#### Minimax Comparison Tournament

Evaluates neural networks against traditional minimax search.

```bash
go run ./alphago_demo/cmd/tournament_with_minimax/main.go [options]
```

Options:
- `-games <n>`: Number of games per agent pair (default: 30)
- `-verbose`: Show detailed output for each move
- `-output <file>`: Output file location (default: output/tournament_with_minimax_results.csv)
- `-max-networks <n>`: Limit number of neural networks of each type (default: 3)

## Training Entry Points

Various training commands are available in `alphago_demo/cmd/`.

#### AlphaGo-Style Model Training (`train_models`)

The primary training entry point for AlphaGo-style models with MCTS.

```bash
go run ./alphago_demo/cmd/train_models/main.go [options]
```

Key Options:
- `-small-run`: Run with reduced parameters for quick testing.
- `-method alphago|neat`: Choose training method (default: alphago).
- `-parallel`: Enable parallel execution.
- `-threads <n>`: Specify number of threads (0 = auto).
- `-m1-*`, `-m2-*`: Configure parameters for two different models trained and compared.
- Run with `-h` to see all options.

#### Extended Training for Top Agents (`train_top_agents`)

Continues training for pre-trained models found in `output/`. Saves results to `output/extended_training/`.

```bash
go run ./alphago_demo/cmd/train_top_agents/main.go [options]
```

Options:
- `-games <n>`: Self-play games for training (default: 2000)
- `-sims <n>`: MCTS simulations per move (default: 400)
- `-tournament-only`: Skip training, run tournament only.
- `-training-only`: Skip tournament, do training only.
- `-output <dir>`: Directory for output files (default: output/extended_training)

#### Supervised Learning from Expert Data (`train_supervised`)

Trains models on existing expert gameplay data (CSV format).

```bash
# (Requires training_data.csv and validation_data.csv)
go run ./alphago_demo/cmd/train_supervised/main.go [options]
```

Options:
- `-hidden <n>`: Hidden layer size (default: 128)
- `-lr <n>`: Learning rate (default: 0.001)
- `-batch <n>`: Batch size (default: 32)
- `-epochs <n>`: Maximum epochs (default: 100)
- `-patience <n>`: Early stopping patience (default: 10)

#### Training Data Generation (`generate_training_data`) (Currently Broken/Legacy)

*Note: This command might be outdated or broken.*
Generates expert gameplay data using minimax search.

```bash
# Potentially broken
go run ./alphago_demo/cmd/generate_training_data/main.go [options]
```

## GPU Acceleration (Optional)

This project supports GPU acceleration through an **optional, separately run** Python gRPC service using ONNX Runtime, providing significantly improved performance for neural network inference (used by benchmarks and potentially some agents).

### Setup

1.  Navigate to the `python/` directory.
2.  Set up the Python environment (using `venv` and `pip`):
    ```bash
    cd python
    python3 -m venv .venv
    source .venv/bin/activate
    pip install -r requirements.txt
    ```
    *(Ensure `requirements.txt` lists `onnxruntime-gpu` or the appropriate package for your GPU/CUDA setup).*
3.  Start the gRPC service:
    ```bash
    python neural_service.py
    ```

### Running Benchmarks with GPU

Once the Python service is running, you can run the Go benchmark tool:

```bash
go run ./cmd/benchmark/main.go
```

The benchmark tool will automatically attempt to connect to the service running on `localhost:50054`.

### Architecture

1.  **Python gRPC Service (`python/neural_service.py`)**: Uses ONNX Runtime for hardware-accelerated inference.
2.  **Go gRPC Client (`pkg/neural/gpu/`)**: Go code that sends inference requests to the Python service.
3.  **Protocol Buffer Interface (`pkg/neural/proto/`)**: Defines the gRPC service and message structure.

## Features

- Neural network implementations with various architectures:
  - PPO (Proximal Policy Optimization) in the golang_implementation
  - AlphaGo-style MCTS (Monte Carlo Tree Search) with neural networks in alphago_demo
- GPU acceleration:
  - Native Metal GPU support for Apple Silicon
  - CUDA support for NVIDIA GPUs
  - Efficient batched operations for high throughput
- Game environments with state tracking
- Tournament systems for comparing agent performance
- Comprehensive testing and documentation

## Requirements

- Go 1.20 or later for the Golang implementation and AlphaGo demos
- C++17 compatible compiler for the C++ implementations
- Eigen3 library for the C++ implementations (matrix operations)
  - On macOS: `brew install eigen`
  - On Ubuntu: `apt-get install libeigen3-dev`
  - On Windows: Download from http://eigen.tuxfamily.org/
- Python 3.8 or later for GPU acceleration (recommended)
- Protocol Buffer compiler (protoc) for GPU acceleration
  - On macOS: `brew install protobuf`
  - On Ubuntu: `apt-get install protobuf-compiler`
- uv Python package manager (recommended for GPU acceleration)
  - Install with: `pip install uv`

## Installation

1. Clone the repository:
```bash
git clone https://github.com/zachbeta/neural_rps.git
cd neural_rps
```

2. Install dependencies:
```bash
# Install all dependencies
make install-deps
```

3. (Optional) Set up GPU acceleration:
```bash
# Install Protocol Buffers compiler
brew install protobuf  # macOS
# or
sudo apt-get install protobuf-compiler  # Ubuntu

# Install uv Python package manager
pip install uv

# Set up Python environment for GPU acceleration
./python/setup_local_env.sh
```

## Implementation Details

Each implementation has its own README with more detailed information:

- [Legacy C++ Implementation](legacy_cpp_implementation/README.md)
- [C++ Implementation](cpp_implementation/README.md)
- [Golang Implementation](golang_implementation/README.md)
- [AlphaGo Demos](alphago_demo/README.md)

## Game Descriptions

### Rock Paper Scissors
The traditional game where Rock beats Scissors, Scissors beats Paper, and Paper beats Rock. The neural network learns to predict and counter opponent moves.

### RPS Card Game
A strategic card game where players place Rock, Paper, or Scissors cards on a 3×3 board. Cards can capture adjacent opponent cards according to RPS rules. The player with the most cards on the board when all cards are played wins.

### Tic-Tac-Toe
The classic game where players take turns placing X or O on a 3×3 grid, trying to get three in a row. This implementation demonstrates AlphaGo-style techniques with Monte Carlo Tree Search.

## License

This project is licensed under the MIT License - see the LICENSE file for details.