C++ Neural Rock Paper Scissors Demo
=================================

This is a simplified demo of the C++ Neural RPS implementation.
The full implementation would include:
- Neural network with weights and biases
- Forward and backward propagation
- ReLU and Softmax activation functions
- Training algorithm

Network Architecture:
-------------------
Input Layer: 9 neurons (game state)
Hidden Layer: 12 neurons (with ReLU activation)
Output Layer: 3 neurons (with Softmax activation)

Game State Representation:
------------------------
The game state is encoded as a 9-dimensional vector:
- First 3 elements: One-hot encoding of player's last move
- Next 3 elements: Cards in hand (normalized)
- Last 3 elements: One-hot encoding of opponent's last move

Training Process:
----------------
1. Initialize weights randomly
2. Collect experience by playing games
3. Update policy using PPO (Proximal Policy Optimization)
4. Repeat until convergence

Example Output:
--------------
If opponent played Rock, neural network predicts: Paper
If opponent played Paper, neural network predicts: Scissors
If opponent played Scissors, neural network predicts: Rock

For the complete implementation, see the main.cpp and NeuralNetwork.cpp files.
